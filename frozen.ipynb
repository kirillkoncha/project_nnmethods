{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# RUBERT","metadata":{}},{"cell_type":"code","source":"import os\nfrom collections import defaultdict, Counter\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport pytorch_lightning as pl\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\nbatch_size=14\n#Мы попробовали еще DeepPavlov/rubert-base-cased-conversational\nmodel_name = 'DeepPavlov/rubert-base-cased'\ntrain = pd.read_csv('../input/ruatd2022/train.csv')\ntest = pd.read_csv('../input/attest/test.csv')\nval = pd.read_csv('../input/atdval/val.csv')\n\nle = LabelEncoder() # закодируем лейблы \nle.fit(train['Class'].values)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(model_name)\n#tokenizer.pad_token = tokenizer.eos_token\ndef collate_fn(input_data):\n    texts, labels = zip(*input_data)\n    labels = torch.LongTensor(labels)\n    inputs = tokenizer(texts, return_tensors='pt', padding='longest', max_length=256, truncation=True)\n    inputs['Class'] = labels\n    return inputs\n\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, data, sort=False, le=None):\n        super().__init__()\n        self.texts = data['Text'].values\n        if 'Class' in data.columns: # если есть разметка\n            assert not data['Class'].isnull().any(), \"Some labels are null\"\n            if le is not None:\n                self.labels = le.transform(data['Class'])\n            else:\n                self.labels = data['Class'].values\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        if hasattr(self, 'labels'):\n            return self.texts[idx], self.labels[idx]\n        else:\n            return self.texts[idx], []\n\nclass Metric: # metric class for storing metrics (accuracy, loss)\n    def __init__(self):\n        self.storage = defaultdict(list)\n    \n    def store(self, **kwargs):\n        for key in kwargs:\n            self.storage[key].append(kwargs[key])\n            \n    def reset(self):\n        self.storage.clear()\n        \n    def log(self):\n        for key in self.storage:\n            self.storage[key] = np.mean(self.storage[key])\n        return self.storage.items()\n        \nclass BertClassifier(pl.LightningModule):\n    def __init__(self, model_name, lr=1e-5, num_labels=2):\n        super().__init__()\n        #self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        #self.bert = AutoModelForCausalLM.from_pretrained(model_name, num_labels=num_labels)\n        self.metric = Metric()\n        self.learning_rate = lr\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.bert.parameters(), lr=self.learning_rate)\n        return optimizer\n        \n    def forward(self, x):\n        return self.bert(**x)\n    \n    def training_step(self, batch, batch_idx):\n        labels = batch.pop('Class')\n        logits = self.bert(**batch).logits\n        loss = F.cross_entropy(logits, labels)\n        predictions = logits.argmax(axis=1)\n        accuracy = torch.mean((predictions == labels).double())\n        self.metric.store(loss=loss.item(), accuracy=accuracy.item())\n        if batch_idx % 100: # every 100 batches - log metrics (mean of last 100 batches)\n            for k,v in self.metric.log():\n                self.log(f'train/{k}', v)\n            self.metric.reset()\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        labels = batch.pop('Class')\n        logits = self.bert(**batch).logits\n        loss = F.cross_entropy(logits, labels)\n        self.log('val/loss', loss)\n        predictions = logits.argmax(axis=1)\n        self.log('val/accuracy', torch.mean((predictions == labels).double()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = TextDataset(train, le=le)\nval = TextDataset(val, le=le)\ntest = TextDataset(test, le=le)\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\nval_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n#tokenizer = BertTokenizer.from_pretrained(model_name).cuda()\n\nmodel = BertClassifier(model_name, num_labels=len(le.classes_)).cuda()\n\nfor name, param in model.named_parameters():\n    if name.startswith(\"bert.encoder.layer.1\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.2\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.3\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.4\"):\n        param.requires_grad = False\n\nversion = f\"{model_name}_binary\"\nlogger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), name='lightning_logs', version=version)\ntrainer = pl.Trainer(\n    logger=logger, \n    gpus=[0],\n    max_epochs=3, \n    num_sanity_val_steps=1\n)\ntrainer.fit(model, train_loader, val_loader)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.cuda()\ndef get_accuracy_and_pred(model, loader): # используйте эту функцию для получения accuracy и предсказаний\n    preds = []\n    model.eval()\n    labels = None\n    accs = 0\n    ns = 0\n    for batch in tqdm(loader):\n        for key in batch:\n            #print(key)\n            batch[key] = batch[key].to(model.device)\n        #print(batch)\n        labels = batch.pop('Class')\n        #print(labels)\n\n        with torch.no_grad():\n            pred = model(batch).logits.argmax(axis=1)\n        #print(pred)\n        #print(labels.size())\n        if labels.size()[1] > 0:\n            #print(labels)\n            accs += torch.sum((pred == labels).double())\n        preds.append(pred.cpu().numpy())\n        ns += len(pred)\n        \n        #print(accs, ns)\n    return accs/ns, np.concatenate(preds)\n\nacc, preds = get_accuracy_and_pred(model, test_loader)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('test_preds_rubert_based_frozen.npy', le.inverse_transform(preds))\nprint(f\"Test accuracy: {acc}\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = np.load('test_preds_rubert_based_frozen.npy', allow_pickle=True)\n\n\ndf = pd.read_csv('../input/sample/rubert-base-cased-conversational_128.csv', index_col=False)\ndf.drop('Class', axis = 1, inplace = True)\n\ndf['Class'] = le.inverse_transform(preds)\ndf.to_csv('submitrubert_bert_frozen', index=False)","metadata":{},"execution_count":null,"outputs":[]}]}