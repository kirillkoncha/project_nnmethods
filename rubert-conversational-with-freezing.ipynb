{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Fine Tuning Roberta\n\n\n\n","metadata":{"id":"8OhO4xlwqExT"}},{"cell_type":"code","source":"# Importing the libraries needed\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport time\nimport seaborn as sns\nimport transformers\nimport json\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import AutoTokenizer, AutoModel\nfrom collections import defaultdict, Counter\nimport logging\nlogging.basicConfig(level=logging.ERROR)","metadata":{"_uuid":"e7b5f5ab6f8f300c8900321a91b9340376c986f2","id":"979OUro5Eac3","execution":{"iopub.status.busy":"2022-02-13T12:35:29.050411Z","iopub.execute_input":"2022-02-13T12:35:29.050766Z","iopub.status.idle":"2022-02-13T12:35:31.861677Z","shell.execute_reply.started":"2022-02-13T12:35:29.050686Z","shell.execute_reply":"2022-02-13T12:35:31.860948Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# RUBERT","metadata":{}},{"cell_type":"code","source":"import os\nfrom collections import defaultdict, Counter\nimport torch\nimport torch.nn.functional as F\nimport pandas as pd\nimport numpy as np\nfrom transformers import BertForSequenceClassification, BertTokenizer\nimport pytorch_lightning as pl\nfrom tqdm.notebook import tqdm\nfrom sklearn.preprocessing import LabelEncoder\nfrom transformers import AutoTokenizer, AutoModelForSequenceClassification\n\n\nbatch_size=14\nmodel_name = 'DeepPavlov/rubert-base-cased-conversational'\n#model_name = 'sberbank-ai/rugpt3large_based_on_gpt2'\ntrain = pd.read_csv('../input/ruatd-dataset/train.csv')\ntest = pd.read_csv('../input/ruatd-dataset/test.csv')\nval = pd.read_csv('../input/ruatd-dataset/val.csv')\n\nle = LabelEncoder() # закодируем лейблы \nle.fit(train['Class'].values)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:30:25.987638Z","iopub.execute_input":"2022-02-13T13:30:25.987907Z","iopub.status.idle":"2022-02-13T13:30:27.428709Z","shell.execute_reply.started":"2022-02-13T13:30:25.987875Z","shell.execute_reply":"2022-02-13T13:30:27.427924Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = BertTokenizer.from_pretrained(model_name)\n#tokenizer.pad_token = tokenizer.eos_token\ndef collate_fn(input_data):\n    texts, labels = zip(*input_data)\n    labels = torch.LongTensor(labels)\n    inputs = tokenizer(texts, return_tensors='pt', padding='longest', max_length=256, truncation=True)\n    inputs['Class'] = labels\n    return inputs\n\nclass TextDataset(torch.utils.data.Dataset):\n    def __init__(self, data, sort=False, le=None):\n        super().__init__()\n        self.texts = data['Text'].values\n        if 'Class' in data.columns: # если есть разметка\n            assert not data['Class'].isnull().any(), \"Some labels are null\"\n            if le is not None:\n                self.labels = le.transform(data['Class'])\n            else:\n                self.labels = data['Class'].values\n        \n    def __len__(self):\n        return len(self.texts)\n    \n    def __getitem__(self, idx):\n        if hasattr(self, 'labels'):\n            return self.texts[idx], self.labels[idx]\n        else:\n            return self.texts[idx], []\n\nclass Metric: # metric class for storing metrics (accuracy, loss)\n    def __init__(self):\n        self.storage = defaultdict(list)\n    \n    def store(self, **kwargs):\n        for key in kwargs:\n            self.storage[key].append(kwargs[key])\n            \n    def reset(self):\n        self.storage.clear()\n        \n    def log(self):\n        for key in self.storage:\n            self.storage[key] = np.mean(self.storage[key])\n        return self.storage.items()\n        \nclass BertClassifier(pl.LightningModule):\n    def __init__(self, model_name, lr=1e-5, num_labels=2):\n        super().__init__()\n        #self.bert = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        self.bert = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)\n        #self.bert = AutoModelForCausalLM.from_pretrained(model_name, num_labels=num_labels)\n        self.metric = Metric()\n        self.learning_rate = lr\n        \n    def configure_optimizers(self):\n        optimizer = torch.optim.AdamW(self.bert.parameters(), lr=self.learning_rate)\n        return optimizer\n        \n    def forward(self, x):\n        return self.bert(**x)\n    \n    def training_step(self, batch, batch_idx):\n        labels = batch.pop('Class')\n        logits = self.bert(**batch).logits\n        loss = F.cross_entropy(logits, labels)\n        predictions = logits.argmax(axis=1)\n        accuracy = torch.mean((predictions == labels).double())\n        self.metric.store(loss=loss.item(), accuracy=accuracy.item())\n        if batch_idx % 100: # every 100 batches - log metrics (mean of last 100 batches)\n            for k,v in self.metric.log():\n                self.log(f'train/{k}', v)\n            self.metric.reset()\n        return loss\n    \n    def validation_step(self, batch, batch_idx):\n        labels = batch.pop('Class')\n        logits = self.bert(**batch).logits\n        loss = F.cross_entropy(logits, labels)\n        self.log('val/loss', loss)\n        predictions = logits.argmax(axis=1)\n        self.log('val/accuracy', torch.mean((predictions == labels).double()))","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:30:37.540165Z","iopub.execute_input":"2022-02-13T13:30:37.540422Z","iopub.status.idle":"2022-02-13T13:30:40.718786Z","shell.execute_reply.started":"2022-02-13T13:30:37.540392Z","shell.execute_reply":"2022-02-13T13:30:40.718014Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = TextDataset(train, le=le)\nval = TextDataset(val, le=le)\ntest = TextDataset(test, le=le)\n\ntrain_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\nval_loader = torch.utils.data.DataLoader(val, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\ntest_loader = torch.utils.data.DataLoader(test, batch_size=batch_size, shuffle=False, collate_fn=collate_fn)\n\n#tokenizer = BertTokenizer.from_pretrained(model_name).cuda()\n\nmodel = BertClassifier(model_name, num_labels=len(le.classes_)).cuda()\n\n#here is freezing layers\nfor name, param in model.named_parameters():\n    if name.startswith(\"bert.encoder.layer.1\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.2\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.3\"):\n        param.requires_grad = False\n    if name.startswith(\"bert.encoder.layer.4\"):\n        param.requires_grad = False\n\nversion = f\"{model_name}_binary\"\nlogger = pl.loggers.TensorBoardLogger(save_dir=os.getcwd(), name='lightning_logs', version=version)\ntrainer = pl.Trainer(\n    logger=logger, \n    gpus=[0],\n    max_epochs=5, \n    num_sanity_val_steps=1\n)\ntrainer.fit(model, train_loader, val_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:31:30.757871Z","iopub.execute_input":"2022-02-13T13:31:30.75816Z","iopub.status.idle":"2022-02-13T15:54:58.53125Z","shell.execute_reply.started":"2022-02-13T13:31:30.758126Z","shell.execute_reply":"2022-02-13T15:54:58.530493Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for name, param in model.named_parameters():\n#      print(name, param.requires_grad)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T13:15:20.416242Z","iopub.execute_input":"2022-02-13T13:15:20.416941Z","iopub.status.idle":"2022-02-13T13:15:20.476401Z","shell.execute_reply.started":"2022-02-13T13:15:20.416903Z","shell.execute_reply":"2022-02-13T13:15:20.475623Z"},"collapsed":true,"jupyter":{"outputs_hidden":true},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.cuda()\ndef get_accuracy_and_pred(model, loader): # используйте эту функцию для получения accuracy и предсказаний\n    preds = []\n    model.eval()\n    labels = None\n    accs = 0\n    ns = 0\n    for batch in tqdm(loader):\n        for key in batch:\n            #print(key)\n            batch[key] = batch[key].to(model.device)\n        #print(batch)\n        labels = batch.pop('Class')\n        #print(labels)\n\n        with torch.no_grad():\n            pred = model(batch).logits.argmax(axis=1)\n        #print(pred)\n        #print(labels.size())\n        if labels.size()[1] > 0:\n            #print(labels)\n            accs += torch.sum((pred == labels).double())\n        preds.append(pred.cpu().numpy())\n        ns += len(pred)\n        \n        #print(accs, ns)\n    return accs/ns, np.concatenate(preds)\n\nacc, preds = get_accuracy_and_pred(model, test_loader)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T15:55:03.07504Z","iopub.execute_input":"2022-02-13T15:55:03.075314Z","iopub.status.idle":"2022-02-13T16:02:44.587903Z","shell.execute_reply.started":"2022-02-13T15:55:03.075284Z","shell.execute_reply":"2022-02-13T16:02:44.587287Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"np.save('test_preds_rubert_based_case_conversational_5ep.npy', le.inverse_transform(preds))\nprint(f\"Test accuracy: {acc}\")","metadata":{"execution":{"iopub.status.busy":"2022-02-13T16:05:35.717352Z","iopub.execute_input":"2022-02-13T16:05:35.717797Z","iopub.status.idle":"2022-02-13T16:05:35.729755Z","shell.execute_reply.started":"2022-02-13T16:05:35.717756Z","shell.execute_reply":"2022-02-13T16:05:35.729061Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_testt = pd.read_csv('../input/ruatd-dataset/test.csv', index_col=False)\ndf_testt.drop('Text', axis = 1, inplace = True)\ndf_testt","metadata":{"execution":{"iopub.status.busy":"2022-02-13T16:15:17.994709Z","iopub.execute_input":"2022-02-13T16:15:17.995538Z","iopub.status.idle":"2022-02-13T16:15:18.551609Z","shell.execute_reply.started":"2022-02-13T16:15:17.995482Z","shell.execute_reply":"2022-02-13T16:15:18.550938Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"f = np.load('test_preds_rubert_based_case_conversational_5ep.npy', allow_pickle=True)\n\n# df = pd.read_csv('sample_submit_binary.csv', index_col=False)\n# df.drop('Class', axis = 1, inplace = True)\n\ndf_testt['Class'] = f\n\ndf_testt.to_csv('test_preds_rubert_based_case_conversational_5ep.csv', index=False)","metadata":{"execution":{"iopub.status.busy":"2022-02-13T16:16:24.600616Z","iopub.execute_input":"2022-02-13T16:16:24.60118Z","iopub.status.idle":"2022-02-13T16:16:24.705973Z","shell.execute_reply.started":"2022-02-13T16:16:24.601145Z","shell.execute_reply":"2022-02-13T16:16:24.705295Z"},"trusted":true},"execution_count":null,"outputs":[]}]}